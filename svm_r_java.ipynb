{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Support Vector Machines  \n",
    "## A Novel Technique for Analytical Classification of Extremely Large Data\n",
    "\n",
    "### Chris Bailey, 17Oct2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a vector $\\vec{w}$ that is perpendicular to the line of maximal separation between the + and - samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for an unknown point $\\vec{u}$ iff \n",
    "$$\\vec{w}\\cdot\\vec{u} \\ge c$$ or equivalently if $$\\vec{w}\\cdot\\vec{u} + b \\ge 0$$ then +, where $b=-c$\n",
    "\n",
    "This is the decision rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we do not know $\\vec{w}$ or $b$ but we know that $\\vec{w}$ is perpendicular to the median line of the \"widest street\" separating the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To separate\n",
    "$$\n",
    "\\vec{w}\\cdot\\vec{x}_{+}+b \\ge 1\n",
    "$$\n",
    "for the positive samples, and, similarly for the negative\n",
    "$$\n",
    "\\vec{w}\\cdot\\vec{x}_{-}+b \\le -1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce a variable \n",
    "$$\n",
    "y_i = \\text{sign}( x_i ) \\quad \\quad \\text{-1 for neg samples, +1 for pos}\n",
    "$$\n",
    "Multiplying the two previous equations by $y_i$\n",
    "$$\n",
    "y_i ( \\vec{w}\\cdot\\vec{x}_{+}+b ) \\ge 1 y_i \n",
    "$$\n",
    "for the positive samples, and, similarly for the negative\n",
    "$$\n",
    "y_i ( \\vec{w}\\cdot\\vec{x}_{-}+b ) \\le -1 y_i\n",
    "$$\n",
    "Since we know the value of $y_i$ they both simplify to the same equation (noting the inequality reverses when multiplied by negation) to\n",
    "$$\n",
    "y_i(\\vec{w}\\cdot \\vec{x}_i+b) \\ge 1\n",
    "$$\n",
    "Bring the 1 over to the left side\n",
    "$$\n",
    "y_i(\\vec{w}\\cdot \\vec{x}_i+b) -1 \\ge 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for $\\vec{x}_i$ that are exactly on the margins, i.e. the support vectors, simplify the equation from inequality to exact, an additional constraint\n",
    "$$\n",
    "y_i(\\vec{w}\\cdot \\vec{x}_i+b) -1 = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe width of the street, using the vectors that lie on the margins $\\vec{x}_{+}$ and $\\vec{x}_{-}$ then\n",
    "$$\n",
    "\\text{Width} = (\\vec{x}_{+} - $\\vec{x}_{-}) \\cdot \\frac{\\vec{w}}{||w||}\n",
    "$$\n",
    "Now by rewriting the previous equation of\n",
    "$$\n",
    "y_i(\\vec{w}\\cdot \\vec{x}_i+b) -1 = 0\n",
    "$$\n",
    "For the positive samples $\\vec{x}_{+}$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_i(\\vec{w}\\cdot \\vec{x}_i+b) -1 &= 0 \\\\\n",
    "y_i &= 1 \\\\\n",
    "\\vec{w}\\cdot \\vec{x}_i+b -1 &= 0 \\\\\n",
    "\\vec{x}_i \\cdot \\vec{w} &= 1 - b \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Similarly for the negative samples $\\vec{x}_{+}$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_i(\\vec{w}\\cdot \\vec{x}_i+b) -1 &= 0 \\\\\n",
    "y_i &= -1 \\\\\n",
    "-\\vec{w}\\cdot \\vec{x}_i-b -1 &= 0 \\\\\n",
    "\\vec{x}_i \\cdot \\vec{w} &= 1 + b \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Pulling the equation at the beginning and substituting\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Width} &= (\\vec{x}_{+} - $\\vec{x}_{-}) \\cdot \\frac{\\vec{w}}{||w||} \\\\\n",
    "\\vec{x}_{+} \\cdot \\vec{w} &= 1-b \\\\\n",
    "-\\vec{x}_{-} \\cdot \\vec{w} &= 1 + b \\\\\n",
    "\\implies \\text{Width} &= \\frac{2}{||\\vec{w}||}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve maximum width find maximum\n",
    "$$\n",
    "\\max \\frac{2}{||\\vec{w}||}\n",
    "$$\n",
    "equivalently\n",
    "$$\n",
    "\\max \\frac{1}{||\\vec{w}||}\n",
    "$$\n",
    "equivalently\n",
    "$$\n",
    "\\min {||\\vec{w}||}\n",
    "$$\n",
    "equivalently, for mathematical convenience\n",
    "$$\n",
    "\\min \\frac{1}{2} {||\\vec{w}||}^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a max/min problem subject to a constraint, so use a LaGrange multiplier and find the extremum \n",
    "$$\n",
    "L = \\frac{1}{2} || \\vec{w} ||^2 - \\sum \\alpha_i \\left[ y_i ( \\vec{w} \\cdot \\vec{x}_i + b ) - 1 \\right]\n",
    "$$\n",
    "Taking the derivative (the derivative of a vector works the same as the derivative of a scalar), first wrt $\\vec{w}$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\vec{w}} = \\vec{w} - \\sum \\alpha_i y_i \\vec{x}_i = 0 \\implies \\vec{w} = \\sum_i \\alpha_i y_i x_i\n",
    "$$\n",
    "Then with respect to $b$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\sum \\alpha_i y_i = 0  \\quad \\quad \\text{note 2: used later}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now substituting in the values from the partial derivatives (not for the maginitude square use the product of $i$ and $j$ (\n",
    "$$\n",
    "L = \\frac{1}{2} \\left( \\sum_i \\alpha_i y_i \\vec{x}_i \\right) \\cdot \\left( \\sum_j \\alpha_i y_i \\vec{x}_j \\right) - \\sum_i \\alpha_i y_i \\vec{x}_i \\cdot \\left( \\sum_j \\alpha_j y_j x_j \\right) - \\sum_i \\alpha_i y_i b + \\sum_i \\alpha_i\n",
    "$$\n",
    "Since $b$ is a constant\n",
    "$$\n",
    "L = \\frac{1}{2} \\left( \\sum_i \\alpha_i y_i \\vec{x}_i \\right) \\cdot \\left( \\sum_j \\alpha_i y_i \\vec{x}_j \\right) - \\sum_i \\alpha_i y_i \\vec{x}_i \\cdot \\left( \\sum_j \\alpha_j y_j x_j \\right) - b \\sum_i \\alpha_i y_i + \\sum_i \\alpha_i\n",
    "$$\n",
    "But from note 2 above the term multiplied by $b$ is 0, so drop the term\n",
    "$$\n",
    "L = \\frac{1}{2} \\left( \\sum_i \\alpha_i y_i \\vec{x}_i \\right) \\cdot \\left( \\sum_j \\alpha_i y_i \\vec{x}_j \\right) - \\sum_i \\alpha_i y_i \\vec{x}_i \\cdot \\left( \\sum_j \\alpha_j y_j x_j \\right) + \\sum_i \\alpha_i\n",
    "$$\n",
    "Since the first two dot products have identical vector components, just add the constants in front\n",
    "$$\n",
    "L = \\sum_i \\alpha_i - \\frac{1}{2} \\left( \\sum_i \\alpha_i y_i \\vec{x}_i \\right) \\cdot \\left( \\sum_j \\alpha_i y_i \\vec{x}_j \\right) \n",
    "$$\n",
    "Combing the summation terms\n",
    "$$\n",
    "L = \\sum_i \\alpha_i - \\frac{1}{2} \\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j \\vec{x}_i \\cdot \\vec{x}_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is predicated on the dot product $\\vec{x}_i \\cdot \\vec{x}_j$\n",
    "And for an unknown $\\vec{u}$\n",
    "$$\n",
    "\\sum \\alpha_i y_i \\vec{x}_i \\cdot \\vec{u} + b \\ge 0 \\quad \\quad \\text{then estimate is }+\n",
    "$$\n",
    "so again a dot product $\\vec{x}_i \\cdot \\vec{u}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An explanation of the iterative convergence algorithm given the quadratic\n",
    "\n",
    "https://en.wikipedia.org/wiki/Interior-point_method#Primal-dual_interior-point_method_for_nonlinear_optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But, need kernel trick\n",
    "\n",
    "this only applies for linearly separable classes where the equations hold\n",
    "\n",
    "To solve when points cross the original hyperplane plane employ the kernel trick, finding some equation such that the + points are in a different plane than the - points\n",
    "$$\n",
    "\\Phi(\\vec{x})\n",
    "$$\n",
    "maximizing\n",
    "$$\n",
    "\\Phi(\\vec{x}_i) \\cdot \\Phi(\\vec{x}_j)\n",
    "$$\n",
    "So the kernel\n",
    "$$\n",
    "K(x_i,x_j) = \\Phi(\\vec{x}_i) \\cdot \\Phi(\\vec{x}_j)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear kernel\n",
    "$$\n",
    "(\\vec{u}\\cdot\\vec{v})^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial Basis Function kernel\n",
    "$$\n",
    "e^{-\\frac{||x_i-x_j||}{\\sigma}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0"
      ],
      "text/latex": [
       "0"
      ],
      "text/markdown": [
       "0"
      ],
      "text/plain": [
       "[1] 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#system('R CMD javareconf')\n",
    "#install.packages('rJava')\n",
    "#install.packages('Matrix')\n",
    "library(\"Matrix\")\n",
    "library(\"rJava\")\n",
    "\n",
    "# .jaddClassPath(\"./target/svm-1.0-SNAPSHOT.jar\")\n",
    "classPath <- \"./target/svm-1.0-SNAPSHOT.jar\"\n",
    ".jinit(classpath = classPath)\n",
    "\n",
    "# use fisher data to verify\n",
    "mat <- sparse.model.matrix(Petal.Length ~ ., data = iris)\n",
    "\n",
    "nRows <- nrow(mat)\n",
    "nColumns <- ncol(mat)\n",
    "rowIndex <- mat@i\n",
    "colBegin <- mat@p\n",
    "matValue <- mat@x\n",
    "\n",
    "obj <- \"com.baileyteam.svm.BLAS\"\n",
    "\n",
    "\n",
    ".jcall(obj, returnSig = \"V\",  method = \"kkt\", nRows, nColumns, rowIndex, colBegin, matValue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For class separation comparison: logistic with dichotomous outcome\n",
    "\n",
    "Logistic\n",
    "$$\n",
    "\\frac{P(y=1|x)}{P(y=0|x)}  =e^z=e^{w^Tx+b}\n",
    "$$\n",
    "Solve for $w$ and $b$\n",
    "$$\n",
    "P(y_i|x_i,w,b)=P(y=1|x_i)^{y^i} \\times P(y=0|x_i)^{1-y_i} = (\\sigma(w^Tx_i+b))^{y_i}\\times (1-\\sigma(w^Tx_i+b))^{1-y_i}\n",
    "$$\n",
    "with the likelihood\n",
    "$$\n",
    "\\mathcal{L}(w,b)=\\prod_{i=1}^n P(y=1|x_i)^{y^i} \\times P(y=0|x_i)^{1-y_i}\n",
    "$$\n",
    "Taking the negative log (for numeric stability) is the cross entropy function\n",
    "$$\n",
    "-\\log \\mathcal{L}(w,b) = -\\sum_{i=1}^n y_i \\log (\\sigma(w^T x_i + b)) + (1-y_i) \\log (1-\\sigma(w^Tx_i + b))\n",
    "$$\n",
    "Need to find model parms $(w^*, b^*)$ resulting in lowest cross entropy\n",
    "$$\n",
    "(w^*, b^*) = \\arg \\min_{(w,b)} -\\log \\mathcal{L}(w,b)\n",
    "$$\n",
    "Using Newton-Raphson iteratively (using a single vector to describe $\\overset{\\sim}{w}=(w^Tb)^T, \\overset{\\sim}{x}=(x^T1)^T, \\sigma=\\sigma(z_1)\\ldots \\sigma(z_n), \\overset{\\sim}{X}=\\overset{\\sim}{x}_1 \\ldots \\overset{\\sim}{x}_n$ )\n",
    "$$\n",
    "\\overset{\\sim}{w}^{(new)}=\\overset{\\sim}{w}^{(old)}-H^{-1}\\bigtriangledown E(\\overset{\\sim}{w})\n",
    "$$\n",
    "The first order derivative\n",
    "$$\n",
    "\\bigtriangledown E(\\overset{\\sim}{w}) = - \\sum_{i=1}^n y_i \\overset{\\sim}{x}_i(1-\\sigma(\\overset{\\sim}{w}^T\\overset{\\sim}{x}_i))-(1-y_i)\\overset{\\sim}{x}_i\\sigma(\\overset{\\sim}{w}^T\\overset{\\sim}{x}_i)\n",
    "$$\n",
    "Taking the second order derivative of $E(\\overset{\\sim}{w})$\n",
    "$$\n",
    "H=\\bigtriangledown \\bigtriangledown E(\\overset{\\sim}{w}) = \\sum_{i=1}^n \\sigma (\\overset{\\sim}{w}^T\\overset{\\sim}{x}_i)(1-\\sigma (\\overset{\\sim}{w}^T\\overset{\\sim}{x}_i))\\overset{\\sim}{x}_i \\overset{\\sim}{x}_i^T = \\overset{\\sim}{X}^TR \\overset{\\sim}{X}\n",
    "$$\n",
    "where $R$ is a diagonal matrix $R_{ii}=\\sigma_i(1-\\sigma_i)$ obtaining the update equation\n",
    "$$\n",
    "\\overset{\\sim}{w}^{(k+1)}=\\overset{\\sim}{w}^k-(\\overset{\\sim}{X}^TR_k\\overset{\\sim}{X})^{-1}\\overset{\\sim}{X}^T(\\sigma_k-y)\n",
    "$$\n",
    "\n",
    "Note QDA, LDA will also outperform logistic given input characteristics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
